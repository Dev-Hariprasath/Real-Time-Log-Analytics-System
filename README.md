# ğŸ“Š Real-Time Log Analytics System

## ğŸš€ Overview

The **Real-Time Log Analytics System** is a scalable platform for **real-time log ingestion, processing, enrichment, and analytics**.
It integrates streaming data from **Kafka** and historical data from **MongoDB**, processes them via **Apache Spark Structured Streaming**, and stores enriched analytics in **PostgreSQL** for visualization and alerting.

### âœ¨ Key Features

* â±ï¸ **Real-time monitoring** of application logs
* ğŸš¨ **Error detection & alerting** (configurable rules)
* âš¡ **Performance insights**: request latency, throughput, error rates
* ğŸ“Š **Interactive dashboards** with **Grafana / Superset / MongoDB Atlas Charts**

---

## ğŸ—ï¸ System Architecture

```text
Kafka (Streaming Logs) ----\
                            >---- Spark Structured Streaming ----> PostgreSQL (Analytics DB) ----> Grafana / Superset ----> Alerts
MongoDB (Historical Logs) --/                      
```

![Architecture](Asserts/Images/dashboard.png)

---

## ğŸ“‚ Project Structure

```text
src/main/scala/com/loganalytics/
  â”œâ”€â”€ controller/
  â”‚    â””â”€â”€ LogStreamController.scala   # Entry point for streaming job
  â”œâ”€â”€ dao/
  â”‚    â”œâ”€â”€ KafkaDAO.scala              # Kafka ingestion layer
  â”‚    â”œâ”€â”€ MongoDAO.scala              # Mongo ingestion & metadata retrieval
  â”‚    â””â”€â”€ PostgresDAO.scala           # PostgreSQL read/write operations
  â”œâ”€â”€ service/
  â”‚    â”œâ”€â”€ LogParserService.scala      # Parse raw JSON logs â†’ structured schema
  â”‚    â”œâ”€â”€ RawLogService.scala         # Normalize raw log schema
  â”‚    â”œâ”€â”€ MongoLogService.scala       # MongoDB schema transformation
  â”‚    â”œâ”€â”€ EnrichmentService.scala     # Service metadata enrichment
  â”‚    â””â”€â”€ UnionService.scala          # Merge live & historical datasets
  â””â”€â”€ utils/
       â””â”€â”€ SchemaUtils.scala           # Centralized schema definitions
```

---

## âš™ï¸ Tech Stack

| Technology                                                                                            | Role                         |
| ----------------------------------------------------------------------------------------------------- | ---------------------------- |
| ![Kafka](https://img.shields.io/badge/Apache%20Kafka-231F20?logo=apache-kafka\&logoColor=white)       | Real-time log ingestion      |
| ![MongoDB](https://img.shields.io/badge/MongoDB-4EA94B?logo=mongodb\&logoColor=white)                 | Historical logs + metadata   |
| ![Spark](https://img.shields.io/badge/Apache%20Spark-E25A1C?logo=apachespark\&logoColor=white)        | ETL, enrichment & analytics  |
| ![PostgreSQL](https://img.shields.io/badge/PostgreSQL-316192?logo=postgresql\&logoColor=white)        | Analytics-ready data sink    |
| ![Scala](https://img.shields.io/badge/Scala-DC322F?logo=scala\&logoColor=white)                       | Data pipeline implementation |
| ![Docker](https://img.shields.io/badge/Docker-2496ED?logo=docker\&logoColor=white)                    | Containerized deployment     |
| ![Charts](https://img.shields.io/badge/MongoDB%20Atlas%20Charts-00ED64?logo=mongodb\&logoColor=white) | Visualization                |

---

## ğŸ“¥ Setup & Installation

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/your-username/Real-Time-Log-Analytics-System.git
cd Real-Time-Log-Analytics-System
```

### 2ï¸âƒ£ Configure Connections

Update credentials in `src/main/resources/application.conf`.

### 3ï¸âƒ£ Start Dependencies

* â–¶ï¸ Start **Kafka broker + Zookeeper**
* ğŸ˜ Start **PostgreSQL** instance
* â˜ï¸ Ensure **MongoDB Atlas** cluster is accessible

### 4ï¸âƒ£ Build & Run

```bash
# Start Zookeeper
.\bin\windows\zookeeper-server-start.bat .\config\zookeeper.properties  

# Start Kafka broker
.\bin\windows\kafka-server-start.bat .\config\server.properties  

# Start Kafka consumer
.\bin\windows\kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic logs --from-beginning
```

Logs will now be streamed in **real time**.

---

## ğŸ“Š Visualization

### Grafana

* Connect PostgreSQL as a data source
* Import **log monitoring dashboards**

![Grafana Dashboard](Asserts/Images/db1.jpg)

### PostgreSQL (PgAdmin)

* Explore normalized log data in **analytics-ready tables**

![PgAdmin Dashboard](Asserts/Images/pgdb.png)

### MongoDB Atlas

* View and query historical logs

![MongoDB Atlas](Asserts/Images/mdb.png)

---

## ğŸ” Example Queries

**Logs per service (hourly aggregation):**

```mongodb
[{
  $group: {
    _id: { service: "$service", hour: { $hour: "$event_time" } },
    count: { $sum: 1 }
  }
}]
```

**Error trend analysis:**

```mongodb
[{
  $match: { level: "ERROR" }
}, {
  $group: {
    _id: { service: "$service", hour: { $hour: "$event_time" } },
    errors: { $sum: 1 }
  }
}]
```

---

## ğŸš¨ Alerts (Optional Extension)

Define **business rules** for anomaly detection:

* âŒ **Error rate > 5% in 10 min** â†’ Trigger alert
* ğŸ•‘ **Latency > 2s** for critical services â†’ Notify via Slack/Email

![Alert Example](Asserts/Images/alert.png)

---

## ğŸ¤ Contributing

Contributions are welcome!

1. ğŸ´ Fork the repository
2. ğŸŒ± Create a feature branch (`git checkout -b feature/new-feature`)
3. âœ… Commit your changes (`git commit -m 'Add new feature'`)
4. ğŸš€ Push to your branch (`git push origin feature/new-feature`)
5. ğŸ” Open a Pull Request

---

## ğŸ“œ License

MIT License Â© 2025
